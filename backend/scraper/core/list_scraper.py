"""
Module de scraping de la liste des offres d'emploi.
"""

import asyncio
import aiohttp
from typing import List, Optional, Dict
from bs4 import BeautifulSoup
from loguru import logger

from ..config.settings import (
    SCRAPING_SOURCES,
    REQUEST_DELAY,
    HTTP_TIMEOUT
)

class JobListScraper:
    """
    Scraper pour la liste des offres d'emploi.
    Parcourt les pages et extrait les URLs des offres de plusieurs sources.
    """

    def __init__(self):
        """Initialise le scraper de liste."""
        self.sources = [s for s in SCRAPING_SOURCES if s['enabled']]
        self.timeout = HTTP_TIMEOUT

    async def _fetch_page(self, url: str) -> Optional[str]:
        """
        R√©cup√®re le contenu HTML d'une page.
        
        Args:
            url: L'URL de la page √† r√©cup√©rer
            
        Returns:
            Optional[str]: Le contenu HTML ou None en cas d'erreur
        """
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=self.timeout) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        logger.warning(f"‚ö†Ô∏è Statut HTTP inattendu: {response.status}")
                        return None
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration de la page: {str(e)}")
            return None

    def _extract_job_urls(self, html: str, source: Dict) -> List[str]:
        """
        Extrait les URLs des offres d'une page.
        
        Args:
            html: Le contenu HTML de la page
            source: Configuration de la source
            
        Returns:
            List[str]: Liste des URLs trouv√©es
        """
        urls = []
        soup = BeautifulSoup(html, 'lxml')
        
        # Log pour debug
        logger.debug(f"üîç HTML re√ßu ({len(html)} caract√®res)")
        logger.debug(f"üéØ Recherche des liens avec le s√©lecteur: {source['selectors']['job_link']}")
        
        # Trouve tous les liens d'offres selon le s√©lecteur de la source
        job_links = soup.select(source['selectors']['job_link'])
        logger.debug(f"üîó Nombre de liens trouv√©s: {len(job_links)}")
        
        for link in job_links:
            href = link.get('href')
            if href:
                # Log pour debug
                logger.debug(f"üìé Lien d'offre trouv√©: {href}")
                
                # Construit l'URL compl√®te si n√©cessaire
                if not href.startswith('http'):
                    base = "https://www.free-work.com"  # URL de base fixe
                    href = f"{base}{href}"
                urls.append(href)
                
        logger.info(f"üìë {len(urls)} offres trouv√©es sur la page")
        return urls

    def _should_continue_to_next_page(self, html: str, source: Dict, current_page_urls: List[str]) -> bool:
        """
        D√©termine s'il faut continuer vers la page suivante.
        La d√©cision est bas√©e sur :
        1. Si la page actuelle a retourn√© des offres
        2. Si un bouton "suivant" est pr√©sent et actif
        
        Args:
            html: Le contenu HTML de la page courante
            source: Configuration de la source
            current_page_urls: URLs trouv√©es sur la page actuelle
            
        Returns:
            bool: True s'il faut continuer vers la page suivante
        """
        # Si la page actuelle n'a pas d'offres, inutile de continuer
        if not current_page_urls:
            logger.debug("üö´ Page sans offres, arr√™t du scraping")
            return False
            
        # V√©rifie la pr√©sence d'un bouton suivant actif
        soup = BeautifulSoup(html, 'lxml')
        next_button = soup.select_one(source['selectors']['next_button'])
        has_next = next_button is not None and not next_button.get('disabled')
        
        logger.debug(f"üìä Page actuelle : {len(current_page_urls)} offres, bouton suivant : {'pr√©sent' if has_next else 'absent'}")
        return has_next

    async def _scrape_source(self, source: Dict) -> List[str]:
        """
        Scrape toutes les offres d'une source.
        
        Args:
            source: Configuration de la source
            
        Returns:
            List[str]: Liste des URLs trouv√©es
        """
        all_urls = []
        page = 1
        continue_scraping = True
        
        logger.info(f"üîç D√©but du scraping de {source['name']}...")
        
        while continue_scraping and page <= source['max_pages']:
            logger.info(f"üìÑ Traitement de la page {page}")
            
            # 1. R√©cup√®re le HTML de la page
            url = f"{source['base_url']}?page={page}"
            html = await self._fetch_page(url)
            if not html:
                break
                
            # 2. Extrait les URLs de la page courante
            page_urls = self._extract_job_urls(html, source)
            all_urls.extend(page_urls)
            
            # 3. D√©termine s'il faut continuer vers la page suivante
            continue_scraping = self._should_continue_to_next_page(html, source, page_urls)
            
            # 4. Passe √† la page suivante si n√©cessaire
            if continue_scraping:
                page += 1
                logger.debug(f"‚è≥ Attente de {REQUEST_DELAY} secondes avant la page suivante...")
                await asyncio.sleep(REQUEST_DELAY)
        
        logger.success(f"‚úÖ Scraping de {source['name']} termin√© : {len(all_urls)} offres trouv√©es")
        return all_urls

    async def get_all_job_urls(self) -> List[str]:
        """
        R√©cup√®re les URLs de toutes les offres de toutes les sources.
        
        Returns:
            List[str]: Liste de toutes les URLs d'offres trouv√©es
        """
        all_urls = []
        
        for source in self.sources:
            try:
                urls = await self._scrape_source(source)
                all_urls.extend(urls)
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du scraping de {source['name']}: {str(e)}")
        
        logger.success(f"‚úÖ Scraping termin√© : {len(all_urls)} offres trouv√©es au total")
        return all_urls 