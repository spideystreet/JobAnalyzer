"""
DAG Airflow pour l'extraction des offres d'emploi.
"""

import asyncio
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from loguru import logger

from backend.scraper.core.list_scraper import JobListScraper
from backend.scraper.core.cache import JobCache
from backend.scraper.config.settings import SCRAPING_INTERVAL

# Configuration par d√©faut du DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2)
}

async def extract_jobs():
    """
    Fonction d'extraction pure qui :
    1. R√©cup√®re les URLs des offres
    2. Extrait le HTML brut
    3. Stocke dans Redis
    """
    try:
        # Initialisation des composants
        list_scraper = JobListScraper()
        cache = JobCache()
        
        logger.info("üöÄ D√©but de l'extraction")
        
        try:
            # R√©cup√®re toutes les URLs
            urls = await list_scraper.get_all_job_urls()
            logger.info(f"üìë {len(urls)} offres trouv√©es au total")
        except Exception as e:
            logger.exception(f"‚ùå Erreur lors de la r√©cup√©ration des URLs: {str(e)}")
            raise
        
        # Compteurs pour les statistiques
        extracted = 0
        skipped = 0
        failed = 0
        
        # Traite chaque URL
        for url in urls:
            try:
                # V√©rifie si l'URL a d√©j√† √©t√© trait√©e
                if await cache.is_processed(url):
                    logger.debug(f"‚è≠Ô∏è URL d√©j√† extraite: {url}")
                    skipped += 1
                    continue
                
                # Extrait uniquement le HTML brut
                html_content = await list_scraper._fetch_page(url)
                
                if html_content:
                    # Stocke le HTML brut dans Redis
                    await cache.store_raw_html(url, html_content)
                    extracted += 1
                    logger.success(f"‚úÖ HTML extrait: {url}")
                else:
                    failed += 1
                    logger.error(f"‚ùå √âchec de l'extraction: {url}")
                    
            except Exception as e:
                failed += 1
                logger.exception(f"‚ùå Erreur lors de l'extraction de {url}: {str(e)}")
        
        # Log des statistiques finales
        logger.success(
            "üìä Bilan de l'extraction:\n"
            f"  - Offres trouv√©es: {len(urls)}\n"
            f"  - HTML extraits: {extracted}\n"
            f"  - D√©j√† vus: {skipped}\n"
            f"  - √âchecs: {failed}"
        )
        
    except Exception as e:
        logger.exception(f"‚ùå Erreur fatale: {str(e)}")
        raise
    finally:
        # Ferme la connexion Redis
        cache.close()

def run_extraction():
    """Point d'entr√©e pour Airflow qui ex√©cute la fonction asynchrone."""
    asyncio.run(extract_jobs())

# Cr√©ation du DAG
dag = DAG(
    'DATA_PIPELINE.01_JOB_SCRAPING',
    default_args=default_args,
    description='Extraction quotidienne du HTML des offres d\'emploi',
    schedule_interval=SCRAPING_INTERVAL,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['ingestion', 'scraping', 'extract', 'step_01']
)

# T√¢che d'extraction
extraction_task = PythonOperator(
    task_id='extract_job_html',
    python_callable=run_extraction,
    dag=dag
) 