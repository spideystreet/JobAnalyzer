"""
DAG Airflow pour la transformation et l'analyse des offres d'emploi.
1. Nettoyage du HTML brut
2. Analyse via DeepSeek
"""

import asyncio
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from loguru import logger
import sys
import os

# Configuration avanc√©e du logger
logger.remove()  # Retire les handlers par d√©faut

# Log dans un fichier d√©di√©
log_file = os.path.join("/opt/airflow/logs/app_logs", f"transform_detailed_{datetime.now().strftime('%Y-%m-%d')}.log")
logger.add(
    log_file,
    rotation="500 MB",
    retention="10 days",
    level="DEBUG",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
    enqueue=True,  # Thread-safe
    backtrace=True,  # D√©tails des erreurs
    diagnose=True,  # Informations de diagnostic
    mode="a"  # Mode append
)

# Log aussi dans stderr pour Airflow
logger.add(
    sys.stderr,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}",
    level="INFO",
    backtrace=True,
    diagnose=True
)

from backend.scraper.core.cache import JobCache
from backend.scraper.core.html_cleaner import HTMLCleaner
from backend.scraper.core.job_analyzer import JobAnalyzer
from backend.scraper.config.settings import SCRAPING_INTERVAL

# Configuration par d√©faut du DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2)
}

async def transform_html():
    """
    Fonction qui nettoie le HTML brut :
    1. R√©cup√®re le HTML brut depuis Redis
    2. Le nettoie
    3. Stocke la version nettoy√©e
    """
    try:
        # Initialisation des composants
        cache = JobCache()
        cleaner = HTMLCleaner()
        
        logger.info("üßπ D√©but du nettoyage HTML")
        
        # R√©cup√®re toutes les cl√©s raw_html
        raw_keys = await cache.get_all_raw_html_keys()
        logger.info(f"üìë {len(raw_keys)} offres √† nettoyer")
        
        # Compteurs pour les statistiques
        processed = 0
        failed = 0
        
        # Traite chaque offre
        for key in raw_keys:
            try:
                # R√©cup√®re le HTML brut
                html_content = await cache.get_raw_html(key)
                if not html_content:
                    logger.warning(f"‚ö†Ô∏è HTML non trouv√© pour {key}")
                    failed += 1
                    continue
                
                # Nettoie le HTML
                cleaned_html = cleaner.clean(html_content)
                
                # Stocke le HTML nettoy√©
                await cache.store_cleaned_html(key, cleaned_html)
                processed += 1
                logger.success(f"‚úÖ Nettoyage r√©ussi: {key}")
                    
            except Exception as e:
                failed += 1
                logger.exception(f"‚ùå Erreur lors du nettoyage de {key}: {str(e)}")
        
        # Log des statistiques finales
        logger.success(
            "üìä Bilan du nettoyage:\n"
            f"  - Offres √† nettoyer: {len(raw_keys)}\n"
            f"  - Nettoyages r√©ussis: {processed}\n"
            f"  - √âchecs: {failed}"
        )
        
    except Exception as e:
        logger.exception(f"‚ùå Erreur fatale lors du nettoyage: {str(e)}")
        raise
    finally:
        cache.close()

async def analyze_jobs():
    """
    Fonction qui analyse les offres nettoy√©es :
    1. R√©cup√®re le HTML nettoy√©
    2. L'analyse avec DeepSeek
    3. Stocke le r√©sultat
    """
    try:
        # Initialisation des composants
        cache = JobCache()
        analyzer = JobAnalyzer()
        
        logger.info("üß† D√©but de l'analyse")
        
        # R√©cup√®re toutes les cl√©s des HTML nettoy√©s
        cleaned_keys = await cache.get_all_cleaned_html_keys()
        logger.info(f"üìë {len(cleaned_keys)} offres √† analyser")
        
        # Compteurs pour les statistiques
        processed = 0
        failed = 0
        
        # Traite chaque offre
        for key in cleaned_keys:
            try:
                # R√©cup√®re le HTML nettoy√©
                cleaned_html = await cache.get_cleaned_html(key)
                if not cleaned_html:
                    logger.warning(f"‚ö†Ô∏è HTML nettoy√© non trouv√© pour {key}")
                    failed += 1
                    continue
                
                # Analyse avec DeepSeek
                analysis = await analyzer.analyze(cleaned_html)
                
                if analysis:
                    # Stocke le r√©sultat
                    await cache.store_analysis(key, analysis)
                    processed += 1
                    logger.success(f"‚úÖ Analyse r√©ussie: {key}")
                else:
                    failed += 1
                    logger.error(f"‚ùå √âchec de l'analyse: {key}")
                    
            except Exception as e:
                failed += 1
                logger.exception(f"‚ùå Erreur lors de l'analyse de {key}: {str(e)}")
        
        # Log des statistiques finales
        logger.success(
            "üìä Bilan de l'analyse:\n"
            f"  - Offres √† analyser: {len(cleaned_keys)}\n"
            f"  - Analyses r√©ussies: {processed}\n"
            f"  - √âchecs: {failed}"
        )
        
    except Exception as e:
        logger.exception(f"‚ùå Erreur fatale lors de l'analyse: {str(e)}")
        raise
    finally:
        cache.close()

def run_transform_html():
    """Point d'entr√©e pour Airflow qui ex√©cute la fonction de transformation."""
    asyncio.run(transform_html())

def run_analyze_jobs():
    """Point d'entr√©e pour Airflow qui ex√©cute la fonction d'analyse."""
    asyncio.run(analyze_jobs())

# Cr√©ation du DAG
dag = DAG(
    'DATA_PIPELINE.02_JOB_TRANSFO',
    default_args=default_args,
    description='Transformation et analyse des offres d\'emploi',
    schedule_interval=SCRAPING_INTERVAL,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['transformation', 'analysis', 'step_02']
)

# T√¢che de transformation HTML
transform_task = PythonOperator(
    task_id='transform_html',
    python_callable=run_transform_html,
    dag=dag
)

# T√¢che d'analyse
analyze_task = PythonOperator(
    task_id='analyze_jobs',
    python_callable=run_analyze_jobs,
    dag=dag
)

# D√©finition de l'ordre des t√¢ches
transform_task >> analyze_task 