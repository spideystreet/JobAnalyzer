"""DAG Airflow pour le chargement des analyses d'offres d'emploi dans Supabase.
Ce DAG est responsable de :
1. R√©cup√©rer les analyses depuis Redis
2. Les charger dans Supabase
"""

import asyncio
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from loguru import logger
import sys
import os

# Configuration avanc√©e du logger
logger.remove()  # Retire les handlers par d√©faut

# Log dans un fichier d√©di√©
log_file = os.path.join("/opt/airflow/logs/app_logs", f"load_detailed_{datetime.now().strftime('%Y-%m-%d')}.log")
logger.add(
    log_file,
    rotation="500 MB",
    retention="10 days",
    level="DEBUG",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
    enqueue=True,  # Thread-safe
    backtrace=True,  # D√©tails des erreurs
    diagnose=True,  # Informations de diagnostic
    mode="a"  # Mode append
)

# Log aussi dans stderr pour Airflow
logger.add(
    sys.stderr,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}",
    level="INFO",
    backtrace=True,
    diagnose=True
)

# Ajout du chemin du projet au PYTHONPATH
sys.path.append("/opt/airflow/")

from backend.scraper.core.cache import JobCache
from backend.scraper.core.storage import JobStorage
from backend.scraper.config.settings import SCRAPING_INTERVAL

# Configuration par d√©faut du DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=1)
}

async def load_analyses():
    """
    Charge les analyses d'offres d'emploi depuis Redis vers Supabase.
    """
    try:
        # Initialisation des composants
        cache = JobCache()
        storage = JobStorage()
        
        logger.info("üì§ D√©but du chargement des analyses vers Supabase")
        
        # R√©cup√©ration des cl√©s des analyses
        analysis_keys = await cache.get_all_analysis_keys()
        logger.info(f"üìä {len(analysis_keys)} analyses √† charger")
        
        # Compteurs pour les statistiques
        success_count = 0
        failure_count = 0
        
        # Traitement de chaque analyse
        for key in analysis_keys:
            try:
                # R√©cup√©ration de l'analyse depuis Redis
                analysis = await cache.get_analysis(key)
                if not analysis:
                    logger.warning(f"‚ö†Ô∏è Analyse non trouv√©e pour {key}")
                    failure_count += 1
                    continue
                
                # Extraction de l'URL depuis la cl√© Redis (format: analysis:URL)
                logger.debug(f"üîç Cl√© Redis: {key}")
                try:
                    # La cl√© est au format "analysis:https://www.free-work.com/..."
                    url = key.split("analysis:", 1)[1]
                    if not url:
                        logger.error(f"‚ùå URL non trouv√©e dans la cl√©: {key}")
                        failure_count += 1
                        continue
                    logger.debug(f"üîó URL extraite: {url}")
                    analysis['URL'] = url
                except Exception as e:
                    logger.error(f"‚ùå Erreur lors de l'extraction de l'URL depuis {key}: {str(e)}")
                    failure_count += 1
                    continue
                
                # Conversion de DURATION_DAYS en None si c'est "None"
                if analysis.get('DURATION_DAYS') == "None":
                    analysis['DURATION_DAYS'] = None
                
                # Chargement dans Supabase
                success = await storage.store_job_analysis(analysis)
                if success:
                    success_count += 1
                    logger.success(f"‚úÖ Analyse charg√©e avec succ√®s : {key}")
                    # Suppression de l'analyse de Redis apr√®s chargement r√©ussi
                    await cache.delete_analysis(key)
                else:
                    failure_count += 1
                    logger.error(f"‚ùå √âchec du chargement de l'analyse : {key}")
                
            except Exception as e:
                failure_count += 1
                logger.exception(f"‚ùå Erreur lors du chargement de {key}: {str(e)}")
        
        # Log des statistiques finales
        logger.success(
            "üìä Bilan du chargement:\n"
            f"  - Analyses √† charger: {len(analysis_keys)}\n"
            f"  - Chargements r√©ussis: {success_count}\n"
            f"  - √âchecs: {failure_count}"
        )
        
    except Exception as e:
        logger.exception(f"‚ùå Erreur fatale lors du chargement: {str(e)}")
        raise
    finally:
        cache.close()

def run_load_analyses():
    """Point d'entr√©e pour Airflow qui ex√©cute la fonction de chargement."""
    asyncio.run(load_analyses())

# Cr√©ation du DAG
dag = DAG(
    'DATA_PIPELINE.03_JOB_LOAD',
    default_args=default_args,
    description='Chargement des analyses d\'offres d\'emploi dans Supabase',
    schedule_interval=SCRAPING_INTERVAL,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['load', 'supabase', 'step_03']
)

# T√¢che de chargement
load_task = PythonOperator(
    task_id='load_analyses',
    python_callable=run_load_analyses,
    dag=dag
) 